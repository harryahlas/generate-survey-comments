{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seqcomments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "15hC4G1-CxA4Lj2GGP8hZxUiepTQ2Tofs",
      "authorship_tag": "ABX9TyNhJObSg3U0QrfgQMTtEni0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harryahlas/generate-survey-comments/blob/master/seq2seqcomments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUHj2DLCaD-"
      },
      "source": [
        "# Generate Survey Comments\r\n",
        "Builds a sequence to sequence model to create comments resembling responses from employee surveys.  Training data (*training_comments.csv*, stored in my personal Google Drive and available on request) was pulled from multiple online sources, mostly *data.world*. I truncated the comments at 1000 characters to facilitate training.\r\n",
        "\r\n",
        "The model is based on the work of George Pipis, link below.\r\n",
        "\r\n",
        "https://pub.towardsai.net/word-level-text-generation-dd61a5a0313d\r\n",
        "\r\n",
        "*Note: runs faster on GPU than TPU*\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7xUE0dOghvn"
      },
      "source": [
        "#### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NViZDC0RBsk",
        "outputId": "80e199c4-c0e5-4826-f8cf-d6e47312801b"
      },
      "source": [
        "# Mount Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7AF3zSjgR-Y"
      },
      "source": [
        "#### Load Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3D-DhJKCVXE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "import tensorflow.keras.utils as ku \r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxROo2z8gOff"
      },
      "source": [
        "#### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRX_FVdHCeOC",
        "outputId": "b2d44dac-9b99-4deb-ec72-e9fa1ac0975e"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "data = open('/content/gdrive/MyDrive/Development/seq2seqcomments/training_comments.csv').read()\r\n",
        "#data = open('comments-not-on-github.txt').read()\r\n",
        "corpus = data.lower().split(\"\\n\")\r\n",
        "tokenizer.fit_on_texts(corpus)\r\n",
        "total_words = len(tokenizer.word_index) + 1\r\n",
        "# create input sequences using list of tokens\r\n",
        "input_sequences = []\r\n",
        "for line in corpus:\r\n",
        " token_list = tokenizer.texts_to_sequences([line])[0]\r\n",
        " for i in range(1, len(token_list)):\r\n",
        "  n_gram_sequence = token_list[:i+1]\r\n",
        "  input_sequences.append(n_gram_sequence)\r\n",
        "# pad sequences \r\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\r\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\r\n",
        "# create predictors and label\r\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\r\n",
        "label = ku.to_categorical(label, num_classes=total_words)\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\r\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\r\n",
        "model.add(Dense(total_words, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\r\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\r\n",
        "model.add(Dense(total_words, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 201, 100)          796400    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 201, 300)          301200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 201, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3982)              402182    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7964)              31720612  \n",
            "=================================================================\n",
            "Total params: 33,380,794\n",
            "Trainable params: 33,380,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EwVnHQ3fk8z"
      },
      "source": [
        "#### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9x9g0ewDTXf",
        "outputId": "beb02ca6-c9e5-40cc-c197-071486da0207"
      },
      "source": [
        "history = model.fit(predictors, label, epochs=50, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.8369 - accuracy: 0.2777\n",
            "Epoch 2/50\n",
            "3180/3180 [==============================] - 161s 51ms/step - loss: 3.8244 - accuracy: 0.2798\n",
            "Epoch 3/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.8178 - accuracy: 0.2815\n",
            "Epoch 4/50\n",
            "3180/3180 [==============================] - 158s 50ms/step - loss: 3.8057 - accuracy: 0.2821\n",
            "Epoch 5/50\n",
            "3180/3180 [==============================] - 161s 50ms/step - loss: 3.7943 - accuracy: 0.2827\n",
            "Epoch 6/50\n",
            "3180/3180 [==============================] - 160s 50ms/step - loss: 3.7796 - accuracy: 0.2842\n",
            "Epoch 7/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.7785 - accuracy: 0.2847\n",
            "Epoch 8/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.7567 - accuracy: 0.2871\n",
            "Epoch 9/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.7423 - accuracy: 0.2882\n",
            "Epoch 10/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.7427 - accuracy: 0.2894\n",
            "Epoch 11/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.7309 - accuracy: 0.2902\n",
            "Epoch 12/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.7350 - accuracy: 0.2896\n",
            "Epoch 13/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.7391 - accuracy: 0.2884\n",
            "Epoch 14/50\n",
            "3180/3180 [==============================] - 158s 50ms/step - loss: 3.7232 - accuracy: 0.2912\n",
            "Epoch 15/50\n",
            "3180/3180 [==============================] - 158s 50ms/step - loss: 3.7226 - accuracy: 0.2897\n",
            "Epoch 16/50\n",
            "3180/3180 [==============================] - 161s 50ms/step - loss: 3.6991 - accuracy: 0.2941\n",
            "Epoch 17/50\n",
            "3180/3180 [==============================] - 160s 50ms/step - loss: 3.6896 - accuracy: 0.2940\n",
            "Epoch 18/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6752 - accuracy: 0.2973\n",
            "Epoch 19/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6676 - accuracy: 0.2968\n",
            "Epoch 20/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6644 - accuracy: 0.2977\n",
            "Epoch 21/50\n",
            "3180/3180 [==============================] - 160s 50ms/step - loss: 3.6559 - accuracy: 0.2982\n",
            "Epoch 22/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6579 - accuracy: 0.2983\n",
            "Epoch 23/50\n",
            "3180/3180 [==============================] - 160s 50ms/step - loss: 3.6777 - accuracy: 0.2952\n",
            "Epoch 24/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6377 - accuracy: 0.3014\n",
            "Epoch 25/50\n",
            "3180/3180 [==============================] - 158s 50ms/step - loss: 3.6296 - accuracy: 0.3010\n",
            "Epoch 26/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6311 - accuracy: 0.3016\n",
            "Epoch 27/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6125 - accuracy: 0.3025\n",
            "Epoch 28/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6110 - accuracy: 0.3032\n",
            "Epoch 29/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.6045 - accuracy: 0.3045\n",
            "Epoch 30/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.5856 - accuracy: 0.3055\n",
            "Epoch 31/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.5940 - accuracy: 0.3055\n",
            "Epoch 32/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.5799 - accuracy: 0.3081\n",
            "Epoch 33/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.5859 - accuracy: 0.3053\n",
            "Epoch 34/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.5822 - accuracy: 0.3066\n",
            "Epoch 35/50\n",
            "3180/3180 [==============================] - 159s 50ms/step - loss: 3.5882 - accuracy: 0.3061\n",
            "Epoch 36/50\n",
            "3180/3180 [==============================] - 160s 50ms/step - loss: 3.5817 - accuracy: 0.3066\n",
            "Epoch 37/50\n",
            "3180/3180 [==============================] - 160s 50ms/step - loss: 3.5707 - accuracy: 0.3079\n",
            "Epoch 38/50\n",
            "3180/3180 [==============================] - 161s 51ms/step - loss: 3.5482 - accuracy: 0.3094\n",
            "Epoch 39/50\n",
            "3180/3180 [==============================] - 161s 50ms/step - loss: 3.5451 - accuracy: 0.3099\n",
            "Epoch 40/50\n",
            "3180/3180 [==============================] - 161s 51ms/step - loss: 3.5616 - accuracy: 0.3083\n",
            "Epoch 41/50\n",
            "3180/3180 [==============================] - 161s 51ms/step - loss: 3.5480 - accuracy: 0.3112\n",
            "Epoch 42/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.5600 - accuracy: 0.3090\n",
            "Epoch 43/50\n",
            "3180/3180 [==============================] - 161s 51ms/step - loss: 3.5714 - accuracy: 0.3069\n",
            "Epoch 44/50\n",
            "3180/3180 [==============================] - 161s 51ms/step - loss: 3.5782 - accuracy: 0.3047\n",
            "Epoch 45/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.5690 - accuracy: 0.3065\n",
            "Epoch 46/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.5675 - accuracy: 0.3065\n",
            "Epoch 47/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.5490 - accuracy: 0.3100\n",
            "Epoch 48/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.5483 - accuracy: 0.3100\n",
            "Epoch 49/50\n",
            "3180/3180 [==============================] - 163s 51ms/step - loss: 3.5222 - accuracy: 0.3115\n",
            "Epoch 50/50\n",
            "3180/3180 [==============================] - 162s 51ms/step - loss: 3.5198 - accuracy: 0.3133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kcd0kZSftFG"
      },
      "source": [
        "#### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjA8xFeNUmpB",
        "outputId": "6cb0d972-2d93-4ddb-bfdc-55123a85fca6"
      },
      "source": [
        "model.save('/content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50')\r\n",
        "#model_backup = model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEM9ilj-f1Db"
      },
      "source": [
        "#### Load Model from Drive *(Optional)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajAAPgCDaSCw"
      },
      "source": [
        "from tensorflow import keras\r\n",
        "model = keras.models.load_model('/content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ3cNAHof9H4"
      },
      "source": [
        "#### Function to Predict Words *print_next_words()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Naj51EqyeVnF"
      },
      "source": [
        "# orig\r\n",
        "def print_next_words(seed_text,number_of_words_to_predict):\r\n",
        "  for _ in range(number_of_words_to_predict):\r\n",
        "   token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "   token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "   #predicted = model.predict_classes(token_list, verbose=0)\r\n",
        "   predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "   output_word = \"\"\r\n",
        "   for word, index in tokenizer.word_index.items():\r\n",
        "    if index == predicted:\r\n",
        "     output_word = word\r\n",
        "     break\r\n",
        "   seed_text += \" \" + output_word\r\n",
        "  print(seed_text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB6ExtjcgFqR"
      },
      "source": [
        "#### Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnbhbwJ2aa3f",
        "outputId": "6ad89a56-b05a-4c85-9002-99f273c5f4eb"
      },
      "source": [
        "#print_next_words(\"My manager has helped me at my job. I have grown and become a better employee. I would like to work in the city\", 50)\r\n",
        "print_next_words(\"My job is terrible! Help me\", 50)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My job is terrible! Help me the same page women seek 5 night of the same boat when they can be able to communicate with someone that would not understand the same boat something is losing students and solar com weeks later it is paying the same boat when they can be a lot of people\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgVK9z_ag19D"
      },
      "source": [
        "seed_text = \"My manager has helped me at my job. I have grown and become a better employee. I wonder why this works. to \""
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OKJwdZIgng4",
        "outputId": "d05c7dad-12e8-4e0c-9e4b-8d4e01bbc1f7"
      },
      "source": [
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "output_word = \"\"\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    if index == predicted:\r\n",
        "      output_word = word\r\n",
        "      break\r\n",
        "seed_text += \" \" + output_word\r\n",
        "print(seed_text)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My work is wonderful. I love my manager. I would change with a lot of the same boat groups are wasting money on the same system i am a lot of people in the same time the same as above i think\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nORTnq376HMC",
        "outputId": "6fb9ffd1-71ca-4514-fc20-bedf70c16692"
      },
      "source": [
        "# Seems to work ok\r\n",
        "seed_text = \"My work is wonderful. I love my manager. I would change\"\r\n",
        "words_to_add = 30 \r\n",
        "for i in range(0,words_to_add):\r\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "  predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "  output_word = \"\"\r\n",
        "  for word, index in tokenizer.word_index.items():\r\n",
        "      if index == predicted:\r\n",
        "        output_word = word\r\n",
        "        break\r\n",
        "  seed_text += \" \" + output_word\r\n",
        "  if i == (words_to_add - 1):\r\n",
        "    print(seed_text)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My work is wonderful. I love my manager. I would change with a lot of the same boat groups are wasting money on the same system i am a lot of people in the same time the same as above i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gexVmgg2-et"
      },
      "source": [
        "def get_new_text(seed_text_input):\r\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text_input])[0]\r\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "  predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "  seed_text_input += \" \" + list(tokenizer.word_index.items())[predicted[0]][0]\r\n",
        "  return(seed_text_input)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hjzhmAe4tgq"
      },
      "source": [
        "seed_text = \"My manager has helped me at my job. I have grown and become a better employee. \""
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA-dUYpM4Syi",
        "outputId": "7931ac64-657c-48d7-eef9-8c7120e85497"
      },
      "source": [
        "seed_text = get_new_text(seed_text)\r\n",
        "print(seed_text)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My manager has helped me at my job. I have grown and become a better employee.  to good as to to\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}