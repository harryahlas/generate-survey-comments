{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seqcomments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "15hC4G1-CxA4Lj2GGP8hZxUiepTQ2Tofs",
      "authorship_tag": "ABX9TyMM/6fb2hWQPc9mJF2itCBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harryahlas/generate-survey-comments/blob/master/seq2seqcomments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUHj2DLCaD-"
      },
      "source": [
        "# Generate Survey Comments\r\n",
        "Builds a sequence to sequence model to create comments resembling responses from employee surveys.  Training data (*training_comments.csv*, stored in my personal Google Drive and available on request) was pulled from multiple online sources, mostly *data.world*. I truncated the comments at 1000 characters to facilitate training.\r\n",
        "\r\n",
        "The model is based on the work of George Pipis, link below.\r\n",
        "\r\n",
        "https://pub.towardsai.net/word-level-text-generation-dd61a5a0313d\r\n",
        "\r\n",
        "*Note: runs faster on CPU than TPU*\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7xUE0dOghvn"
      },
      "source": [
        "#### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NViZDC0RBsk",
        "outputId": "e3e7ee7c-cdcc-4e09-f6e6-4482b118bac6"
      },
      "source": [
        "# Mount Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7AF3zSjgR-Y"
      },
      "source": [
        "#### Load Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3D-DhJKCVXE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "import tensorflow.keras.utils as ku \r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxROo2z8gOff"
      },
      "source": [
        "#### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRX_FVdHCeOC",
        "outputId": "4824043d-3d7b-4578-f314-19dff1c0b48f"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "data = open('/content/gdrive/MyDrive/Development/seq2seqcomments/training_comments.csv').read()\r\n",
        "#data = open('comments-not-on-github.txt').read()\r\n",
        "corpus = data.lower().split(\"\\n\")\r\n",
        "tokenizer.fit_on_texts(corpus)\r\n",
        "total_words = len(tokenizer.word_index) + 1\r\n",
        "# create input sequences using list of tokens\r\n",
        "input_sequences = []\r\n",
        "for line in corpus:\r\n",
        " token_list = tokenizer.texts_to_sequences([line])[0]\r\n",
        " for i in range(1, len(token_list)):\r\n",
        "  n_gram_sequence = token_list[:i+1]\r\n",
        "  input_sequences.append(n_gram_sequence)\r\n",
        "# pad sequences \r\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\r\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\r\n",
        "# create predictors and label\r\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\r\n",
        "label = ku.to_categorical(label, num_classes=total_words)\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\r\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\r\n",
        "model.add(Dense(total_words, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\r\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\r\n",
        "model.add(Dense(total_words, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 201, 100)          796400    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 201, 300)          301200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 201, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3982)              402182    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7964)              31720612  \n",
            "=================================================================\n",
            "Total params: 33,380,794\n",
            "Trainable params: 33,380,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EwVnHQ3fk8z"
      },
      "source": [
        "#### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9x9g0ewDTXf",
        "outputId": "591422d3-1626-424f-d747-89980ab682d4"
      },
      "source": [
        "history = model.fit(predictors, label, epochs=15, verbose=1)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3180/3180 [==============================] - 167s 52ms/step - loss: 4.6916 - accuracy: 0.2045\n",
            "Epoch 2/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.6592 - accuracy: 0.2068\n",
            "Epoch 3/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.6357 - accuracy: 0.2078\n",
            "Epoch 4/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.5968 - accuracy: 0.2104\n",
            "Epoch 5/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.5652 - accuracy: 0.2131\n",
            "Epoch 6/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.5369 - accuracy: 0.2149\n",
            "Epoch 7/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.5090 - accuracy: 0.2179\n",
            "Epoch 8/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.4806 - accuracy: 0.2195\n",
            "Epoch 9/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.4504 - accuracy: 0.2210\n",
            "Epoch 10/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.4247 - accuracy: 0.2243\n",
            "Epoch 11/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.3937 - accuracy: 0.2277\n",
            "Epoch 12/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.3749 - accuracy: 0.2289\n",
            "Epoch 13/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.3536 - accuracy: 0.2295\n",
            "Epoch 14/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.3250 - accuracy: 0.2331\n",
            "Epoch 15/15\n",
            "3180/3180 [==============================] - 166s 52ms/step - loss: 4.3066 - accuracy: 0.2335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kcd0kZSftFG"
      },
      "source": [
        "#### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjA8xFeNUmpB",
        "outputId": "b43dc1a8-99c7-4b18-8c33-b761f66c6fea"
      },
      "source": [
        "model.save('/content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50')\r\n",
        "#model_backup = model"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEM9ilj-f1Db"
      },
      "source": [
        "#### Load Model from Drive *(Optional)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajAAPgCDaSCw"
      },
      "source": [
        "from tensorflow import keras\r\n",
        "model = keras.models.load_model('/content/gdrive/My Drive/Development/seq2seqcomments/seq2seq50')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ3cNAHof9H4"
      },
      "source": [
        "#### Function to Predict Words *print_next_words()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Naj51EqyeVnF"
      },
      "source": [
        "# orig\r\n",
        "def print_next_words(seed_text,number_of_words_to_predict):\r\n",
        "  for _ in range(number_of_words_to_predict):\r\n",
        "   token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "   token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "   #predicted = model.predict_classes(token_list, verbose=0)\r\n",
        "   predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "   output_word = \"\"\r\n",
        "   for word, index in tokenizer.word_index.items():\r\n",
        "    if index == predicted:\r\n",
        "     output_word = word\r\n",
        "     break\r\n",
        "   seed_text += \" \" + output_word\r\n",
        "  print(seed_text)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB6ExtjcgFqR"
      },
      "source": [
        "#### Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnbhbwJ2aa3f",
        "outputId": "f9187e8b-8d0f-4f29-ddd2-edea0d946871"
      },
      "source": [
        "print_next_words(\"My manager has helped me at my job. I have grown and become a better employee.\", 50)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My manager has helped me at my job. I have grown and become a better employee. the city of the same as well i am concerned that is a lot of people who are not a lot of people who are not a lot of people who have to get the same as a lot of the same as well as a lot of women are\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgVK9z_ag19D"
      },
      "source": [
        "seed_text = \"My manager has helped me at my job. I have grown and become a better employee. I wonder why this works. to \""
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OKJwdZIgng4",
        "outputId": "d05c7dad-12e8-4e0c-9e4b-8d4e01bbc1f7"
      },
      "source": [
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "output_word = \"\"\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    if index == predicted:\r\n",
        "      output_word = word\r\n",
        "      break\r\n",
        "seed_text += \" \" + output_word\r\n",
        "print(seed_text)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My work is wonderful. I love my manager. I would change with a lot of the same boat groups are wasting money on the same system i am a lot of people in the same time the same as above i think\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nORTnq376HMC",
        "outputId": "6fb9ffd1-71ca-4514-fc20-bedf70c16692"
      },
      "source": [
        "# Seems to work ok\r\n",
        "seed_text = \"My work is wonderful. I love my manager. I would change\"\r\n",
        "words_to_add = 30 \r\n",
        "for i in range(0,words_to_add):\r\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "  predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "  output_word = \"\"\r\n",
        "  for word, index in tokenizer.word_index.items():\r\n",
        "      if index == predicted:\r\n",
        "        output_word = word\r\n",
        "        break\r\n",
        "  seed_text += \" \" + output_word\r\n",
        "  if i == (words_to_add - 1):\r\n",
        "    print(seed_text)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My work is wonderful. I love my manager. I would change with a lot of the same boat groups are wasting money on the same system i am a lot of people in the same time the same as above i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gexVmgg2-et"
      },
      "source": [
        "def get_new_text(seed_text_input):\r\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text_input])[0]\r\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "  predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "  seed_text_input += \" \" + list(tokenizer.word_index.items())[predicted[0]][0]\r\n",
        "  return(seed_text_input)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hjzhmAe4tgq"
      },
      "source": [
        "seed_text = \"My manager has helped me at my job. I have grown and become a better employee. \""
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA-dUYpM4Syi",
        "outputId": "7931ac64-657c-48d7-eef9-8c7120e85497"
      },
      "source": [
        "seed_text = get_new_text(seed_text)\r\n",
        "print(seed_text)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My manager has helped me at my job. I have grown and become a better employee.  to good as to to\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}